{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aba32b65",
      "metadata": {
        "id": "aba32b65"
      },
      "source": [
        "### 1. Importing all the libraries\n",
        "### Model used - Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6ffcbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe6ffcbe",
        "outputId": "cfcd754f-4118-4a5e-8c55-cf7b50eddad3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "!pip install plotly\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e834bc",
      "metadata": {
        "id": "a6e834bc"
      },
      "source": [
        "### 2. Read the CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F7mO-Z820K44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7mO-Z820K44",
        "outputId": "f3ab1b2d-c83e-4998-fa27-3222d6cf1c8b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n5W9jToR6W88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5W9jToR6W88",
        "outputId": "97c41208-082a-4180-dd25-daa762e485bd"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train_preprocessed')\n",
        "df_train.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ce5bf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "65ce5bf1",
        "outputId": "c85411d5-45b8-4e94-a62b-4a8e2d3d6df8"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/_COURSES_/PBA/Preprocessing/train_preprocessed')\n",
        "df_train.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dRlW6MtCIxz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "5dRlW6MtCIxz",
        "outputId": "355b2556-3876-41ba-b39a-9d357e7f6ce3"
      },
      "outputs": [],
      "source": [
        "df_train.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hruxbuz0CIxz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "hruxbuz0CIxz",
        "outputId": "4e6b32c1-48b6-4834-c2a1-dbba3011fb80"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/_COURSES_/PBA/Preprocessing/test_preprocessed')\n",
        "df_test.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tATznSQ46a3N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tATznSQ46a3N",
        "outputId": "d1ebe61c-a2f8-4b66-9684-76f0805e3089"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('test_preprocessed')\n",
        "df_test.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xsf6ZbV5CIxz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "Xsf6ZbV5CIxz",
        "outputId": "9f843a82-e02c-44dd-cf97-f2890f2a77d8"
      },
      "outputs": [],
      "source": [
        "df_test.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PwKKfBnDh3mB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PwKKfBnDh3mB",
        "outputId": "00099a5e-0dac-43c3-9504-4def58189d53"
      },
      "outputs": [],
      "source": [
        "df_train.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VKrEniWdCIxz",
      "metadata": {
        "id": "VKrEniWdCIxz"
      },
      "source": [
        "### 3. Model Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EWBY9t3ED-lm",
      "metadata": {
        "id": "EWBY9t3ED-lm"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime format\n",
        "df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "df_test['date'] = pd.to_datetime(df_test['date'])\n",
        "\n",
        "# set 'sales' as target variable\n",
        "y_train = df_train['sales']\n",
        "X_train = df_train.drop('sales', axis=1)\n",
        "\n",
        "y_test = df_test['sales']\n",
        "X_test = df_test.drop('sales', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7K2l-KaQCIxz",
      "metadata": {
        "id": "7K2l-KaQCIxz"
      },
      "source": [
        "Summary of the steps below:\n",
        "\n",
        "1. We choose 'special_offer','day of week', 'store_nbr','encoded_product_type','sales_lag365','sales_lag21','rolling_means7' as the predictors to the sales.\n",
        "\n",
        "2. Set the training and test dataframe (usually we will split the datasets in this phase, yet we already did that during EDA phase).\n",
        "\n",
        "3. Initialize the model. It initialize the RandomForestRegressor (RFR) model with 100 estimators, a max feature 0.5, and a random state of 42 for reproducibility. This number is decided using GridSearch (library to tuning the best hyperparameter), the process of modelling and parameter tuning is a back and forth process.\n",
        "\n",
        "4. Fits the model: The model is trained using the fit method on the training data.\n",
        "\n",
        "5. Makes predictions: The trained model is used to make predictions on both the training and testing data.\n",
        "\n",
        "6. The Mean Absolute Error (MAE) is calculated for both the training and testing predictions. This is a measure of the differences between the predicted and actual values.\n",
        "\n",
        "BRIEF CONCLUSION! Based MAE & MAPE score below, the model demonstrates relatively good performance as indicated by relatively low MAE values on the training and test datasets, suggesting it captures a significant portion of the variance in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kquo33dkCIxz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kquo33dkCIxz",
        "outputId": "e9765fe4-71a6-4c96-e571-4a3b0713d274"
      },
      "outputs": [],
      "source": [
        "# Select the columns\n",
        "X_train_all = df_train[['special_offer','day of week', 'store_nbr' ,'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']].dropna()\n",
        "y_train = df_train.loc[X_train_all.index, 'sales']\n",
        "X_test_all = df_test[['special_offer', 'day of week', 'store_nbr' ,'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']].dropna()\n",
        "y_test = df_test.loc[X_test_all.index, 'sales']\n",
        "\n",
        "# Initialize the model\n",
        "model = RandomForestRegressor(n_estimators=100, max_features=0.1, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train_all, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_preds = model.predict(X_train_all)\n",
        "test_preds = model.predict(X_test_all)\n",
        "\n",
        "# Calculate MAE\n",
        "train_mae = mean_absolute_error(y_train, train_preds)\n",
        "test_mae = mean_absolute_error(y_test, test_preds)\n",
        "\n",
        "# Calculate RMSE\n",
        "train_rmse = mean_squared_error(y_train, train_preds, squared=False)\n",
        "test_rmse = mean_squared_error(y_test, test_preds, squared=False)\n",
        "\n",
        "# Calculate R2 scores\n",
        "train_r2 = r2_score(y_train, train_preds)\n",
        "test_r2 = r2_score(y_test, test_preds)\n",
        "\n",
        "# Calculate Adjusted R2 scores\n",
        "n_train = len(y_train)\n",
        "p_train = X_train_all.shape[1]\n",
        "train_adj_r2 = 1 - (1 - train_r2) * (n_train - 1) / (n_train - p_train - 1)\n",
        "\n",
        "n_test = len(y_test)\n",
        "p_test = X_test_all.shape[1]\n",
        "test_adj_r2 = 1 - (1 - test_r2) * (n_test - 1) / (n_test - p_test - 1)\n",
        "\n",
        "print(f'MAE on train set: {train_mae}')\n",
        "print(f'MAE on test set: {test_mae}\\n')\n",
        "print(f'RMSE on train set: {train_rmse}')\n",
        "print(f'RMSE on test set: {test_rmse}\\n')\n",
        "print(f'Adjusted R2 on train set: {train_adj_r2}')\n",
        "print(f'Adjusted R2 on test set: {test_adj_r2}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AfrOJ1DJCIx0",
      "metadata": {
        "id": "AfrOJ1DJCIx0"
      },
      "source": [
        "Code below running 1 operations:\n",
        "\n",
        "1. Sorting the feature by its importance to the model performance, resulting that rolling_means7 as the most important feature, followed by sales_lag21, sales_lag365, special_offer, and day_of_week.\n",
        "\n",
        "BRIEF CONCLUSION!\n",
        "rolling_means7 become the most important and suggesting that the rolling mean (moving average) over a 7-day period has the most substantial influence on predicting sales (generally on all stores). Morever, sales lagged by a 3-week also seems to be a significant factor in determining future sales patterns, meaning that the day of the previous 3-week pattern become one of the most important trend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LfHuteapCIx0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "LfHuteapCIx0",
        "outputId": "05e3532d-ee1c-4a27-e29f-802da3c3913c"
      },
      "outputs": [],
      "source": [
        "#feature importance, then sorted by the importance value\n",
        "\n",
        "df_imp = pd.DataFrame(dict(Feature=X_train_all.columns, Importance = model.feature_importances_))\n",
        "df_imp.sort_values(by=\"Importance\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zesVwfi6CIx0",
      "metadata": {
        "id": "zesVwfi6CIx0"
      },
      "source": [
        "### 4. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bkjdh4avCIx0",
      "metadata": {
        "id": "bkjdh4avCIx0"
      },
      "source": [
        "Code below running 1 operations:\n",
        "\n",
        "1. RandomizedSearchCV, this is a parameter tuning operation. As explained before, this operation is usually run after we create the first model and did not sure yet whether the previous parameters were already the best or not. Using 24 parameter combinations, the results are recommending us to set the RFR model at {'max_features': 0.1, 'n_estimators': 100, max_depth=15}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l7NJs-AXHuBn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7NJs-AXHuBn",
        "outputId": "916d8c7c-9086-4b53-cbe1-766660b9d16e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = dict(n_estimators=[50,100],\n",
        "                  max_features=np.arange(0.1,1,0.5),\n",
        "                  max_depth=[10,15])\n",
        "\n",
        "combs = 1\n",
        "for params in param_grid.values():\n",
        "    combs *= len(params)\n",
        "print(combs, \"parameter combinations being tested by Random Search:\")\n",
        "param_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LATR0u87HzCQ",
      "metadata": {
        "id": "LATR0u87HzCQ"
      },
      "outputs": [],
      "source": [
        "tscv = TimeSeriesSplit(n_splits=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D6yM6GG9H0pZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6yM6GG9H0pZ",
        "outputId": "b01bb5e6-a916-4c89-d65c-24fd8af523ed"
      },
      "outputs": [],
      "source": [
        "rs = RandomizedSearchCV(RandomForestRegressor(), param_grid, cv=tscv, n_jobs=-1, n_iter=8)\n",
        "rs.fit(X_train, y_train)\n",
        "print(\"Best parameters found:\", rs.best_params_)\n",
        "print(\"Mean CV score of best parameters:\", rs.best_score_)\n",
        "# Before calculating the score, the model is refit using all training data.\n",
        "print(\"Score of best parameters on test data:\", rs.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m7f_LcKMH09k",
      "metadata": {
        "id": "m7f_LcKMH09k"
      },
      "source": [
        "Re-tuning the Hyperparameter on the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RlncSSoyGDr2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlncSSoyGDr2",
        "outputId": "d0251e07-91c6-4c2f-b8fb-3444990cf24a"
      },
      "outputs": [],
      "source": [
        "# Select the columns\n",
        "X_train = df_train[['special_offer', 'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']].dropna()\n",
        "y_train = df_train.loc[X_train.index, 'sales']\n",
        "X_test = df_test[['special_offer', 'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']].dropna()\n",
        "y_test = df_test.loc[X_test.index, 'sales']\n",
        "\n",
        "# Initialize the model\n",
        "model = RandomForestRegressor(n_estimators=50, max_features=0.6,max_depth=10, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_preds = model.predict(X_train)\n",
        "test_preds = model.predict(X_test)\n",
        "\n",
        "# Calculate MAE\n",
        "train_mae = mean_absolute_error(y_train, train_preds)\n",
        "test_mae = mean_absolute_error(y_test, test_preds)\n",
        "\n",
        "# Calculate RMSE\n",
        "train_rmse = mean_squared_error(y_train, train_preds, squared=False)\n",
        "test_rmse = mean_squared_error(y_test, test_preds, squared=False)\n",
        "\n",
        "# Calculate R2 scores\n",
        "train_r2 = r2_score(y_train, train_preds)\n",
        "test_r2 = r2_score(y_test, test_preds)\n",
        "\n",
        "# Calculate Adjusted R2 scores\n",
        "n_train = len(y_train)\n",
        "p_train = X_train.shape[1]\n",
        "train_adj_r2 = 1 - (1 - train_r2) * (n_train - 1) / (n_train - p_train - 1)\n",
        "\n",
        "n_test = len(y_test)\n",
        "p_test = X_test.shape[1]\n",
        "test_adj_r2 = 1 - (1 - test_r2) * (n_test - 1) / (n_test - p_test - 1)\n",
        "\n",
        "print(f'MAE on train set: {train_mae}')\n",
        "print(f'MAE on test set: {test_mae}\\n')\n",
        "print(f'RMSE on train set: {train_rmse}')\n",
        "print(f'RMSE on test set: {test_rmse}\\n')\n",
        "print(f'Adjusted R2 on train set: {train_adj_r2}')\n",
        "print(f'Adjusted R2 on test set: {test_adj_r2}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gLFtuYeAH6Rk",
      "metadata": {
        "id": "gLFtuYeAH6Rk"
      },
      "source": [
        "Re-running the feature importance after hyperparameter tuning to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_S8tPBvWGUiT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_S8tPBvWGUiT",
        "outputId": "e785b9dd-162b-41cb-cac6-55a14c222050"
      },
      "outputs": [],
      "source": [
        "#feature importance, then sorted by the importance value\n",
        "\n",
        "df_imp = pd.DataFrame(dict(Feature=X_train.columns, Importance = model.feature_importances_))\n",
        "df_imp.sort_values(by=\"Importance\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92Z22fCQCIx0",
      "metadata": {
        "id": "92Z22fCQCIx0"
      },
      "source": [
        "### 5. Making Prediction at Training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2BEUgp4CIx0",
      "metadata": {
        "id": "e2BEUgp4CIx0"
      },
      "source": [
        "Code below running 2 operations:\n",
        "\n",
        "1. Makes prediction for training datasets on the special_offer', 'day of week', 'store_nbr','encoded_product_type','sales_lag365','sales_lag21','rolling_means7' as predictors to 'predicted_sales' as target\n",
        "\n",
        "2. Displays the date, predicted value of each product at each store: It prints out the ‘date’, ‘store_nbr’, ‘product_type’, ‘sales’, and ‘predicted_sales’ columns of the df_train dataframe. This gives us a view of the actual and predicted sales for each product at each store on each date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AUd2oRltCIx0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUd2oRltCIx0",
        "outputId": "7fc2241b-a813-4af3-f2ff-3127388f63f9"
      },
      "outputs": [],
      "source": [
        "df_train.fillna(0, inplace=True)\n",
        "df_test.fillna(0, inplace=True)\n",
        "\n",
        "# Make predictions for the test set\n",
        "test_preds = model.predict(df_train[['special_offer','encoded_product_type','sales_lag365','sales_lag21','rolling_means7']])\n",
        "\n",
        "# Add the predictions to the DataFrame\n",
        "df_train['predicted_sales'] = test_preds\n",
        "\n",
        "# Display the date, predicted value of each product at each store\n",
        "print(df_train[['date', 'store_nbr', 'encoded_product_type','sales', 'predicted_sales']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A0thrXN0CIx1",
      "metadata": {
        "id": "A0thrXN0CIx1"
      },
      "source": [
        "### 6. Setting Predicted sales of Training dataset to Plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vJKmLdDJCIx1",
      "metadata": {
        "id": "vJKmLdDJCIx1"
      },
      "outputs": [],
      "source": [
        "train_result_agg = df_train.set_index('date').resample('D')[['sales', 'predicted_sales']].mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y7wTMSFeCIx1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Y7wTMSFeCIx1",
        "outputId": "6c631c92-6592-4add-cff2-b7ce3e18a2d9"
      },
      "outputs": [],
      "source": [
        "fig = px.line(train_result_agg, x='date', y='sales', title='Actual vs Prediction Sales on Train dataset')\n",
        "fig.add_scatter(x=train_result_agg['date'], y=train_result_agg['predicted_sales'], mode='lines', name='predicted')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-PIRBKv-CIx1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-PIRBKv-CIx1",
        "outputId": "fd20a556-a6b7-47bc-e9fb-fe1bf5be8ff0"
      },
      "outputs": [],
      "source": [
        "store_train = df_train.groupby(['store_nbr', 'date'])[['sales', 'predicted_sales']].mean().reset_index()\n",
        "store_df = store_train\n",
        "fig = make_subplots(rows=18, cols=3, subplot_titles=[f'Store {store}' for store in store_train.store_nbr.unique()])\n",
        "n=1\n",
        "for row in range (1,19):\n",
        "  for col in range(1,4):\n",
        "    df = store_df[store_df['store_nbr'] == n]\n",
        "    n += 1\n",
        "\n",
        "    px_fig = px.line(df, x='date', y='sales')\n",
        "    px_fig.add_scatter(x=df['date'], y=df['predicted_sales'], mode='lines')\n",
        "\n",
        "    for trace in px_fig['data']:\n",
        "      fig.add_trace(trace, row=row, col=col)\n",
        "\n",
        "fig.update_layout(height=4000, width=2000, title_text = 'Sales Prediction by Stores on Train dataset')\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e8CiQBCIx1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28e8CiQBCIx1",
        "outputId": "409fa5cf-1008-4a0c-8e99-58f942992a56"
      },
      "outputs": [],
      "source": [
        "product_test = df_train.groupby(['encoded_product_type', 'product_type', 'date'])[['sales', 'predicted_sales']].mean().reset_index()\n",
        "product_df = product_test\n",
        "fig = make_subplots(rows=11, cols=3, subplot_titles=[f'{product}' for product in product_test.product_type.unique()])\n",
        "n=0\n",
        "for row in range (1,12):\n",
        "  for col in range(1,4):\n",
        "    df = product_df[product_df['encoded_product_type'] == n]\n",
        "    n += 1\n",
        "\n",
        "    px_fig = px.line(df, x='date', y='sales')\n",
        "    px_fig.add_scatter(x=df['date'], y=df['predicted_sales'], mode='lines')\n",
        "\n",
        "    for trace in px_fig['data']:\n",
        "      fig.add_trace(trace, row=row, col=col)\n",
        "\n",
        "fig.update_layout(height=4000, width=2000, title_text = 'Sales Prediction by Product on Train dataset')\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5acddd5f",
      "metadata": {
        "id": "5acddd5f"
      },
      "source": [
        "### --------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1_fiM_OqCIx1",
      "metadata": {
        "id": "1_fiM_OqCIx1"
      },
      "source": [
        "### 7. Making Prediction at Test dataset (31 July - 15 August)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aFjqncrCIx1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aFjqncrCIx1",
        "outputId": "7b7391f1-3c27-4d16-81a3-ff4dab6843ea"
      },
      "outputs": [],
      "source": [
        "# Fill NaN values with 0\n",
        "df_train.fillna(0, inplace=True)\n",
        "df_test.fillna(0, inplace=True)\n",
        "\n",
        "# Make predictions for the test set\n",
        "test_preds = model.predict(df_test[['special_offer', 'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']])\n",
        "\n",
        "# Add the predictions to the DataFrame\n",
        "df_test['predicted_sales'] = test_preds\n",
        "\n",
        "# Display the date, predicted value of each product at each store\n",
        "print(df_test[['date', 'store_nbr', 'encoded_product_type','sales', 'predicted_sales']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "udl08rRGFdN3",
      "metadata": {
        "id": "udl08rRGFdN3"
      },
      "outputs": [],
      "source": [
        "test_result_agg = df_test.set_index('date').resample('D')[['sales', 'predicted_sales']].mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6yxIGTdFeZ6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "_6yxIGTdFeZ6",
        "outputId": "22714981-1739-43cf-8ea0-96338a45dc1a"
      },
      "outputs": [],
      "source": [
        "fig2 = px.line(test_result_agg, x='date', y='sales', title='Actual vs Prediction Sales on Test dataset')\n",
        "fig2.add_scatter(x=test_result_agg['date'], y=test_result_agg['predicted_sales'], mode='lines', name='predicted')\n",
        "fig2.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w2qLam4lCIx1",
      "metadata": {
        "id": "w2qLam4lCIx1"
      },
      "source": [
        "### 8. Setting Predicted sales of Test dataset to Plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MjM9MPCiCIx1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MjM9MPCiCIx1",
        "outputId": "6070f495-9191-438e-f5c5-c5e1e39056f6"
      },
      "outputs": [],
      "source": [
        "store_train = df_test.groupby(['store_nbr', 'date'])[['sales', 'predicted_sales']].mean().reset_index()\n",
        "store_df = store_train\n",
        "fig = make_subplots(rows=18, cols=3, subplot_titles=[f'Store {store}' for store in store_train.store_nbr.unique()])\n",
        "n=1\n",
        "for row in range (1,19):\n",
        "  for col in range(1,4):\n",
        "    df = store_df[store_df['store_nbr'] == n]\n",
        "    n += 1\n",
        "\n",
        "    px_fig = px.line(df, x='date', y='sales')\n",
        "    px_fig.add_scatter(x=df['date'], y=df['predicted_sales'], mode='lines')\n",
        "\n",
        "    for trace in px_fig['data']:\n",
        "      fig.add_trace(trace, row=row, col=col)\n",
        "\n",
        "fig.update_layout(height=4000, width=2000, title_text = 'Sales Prediction by Stores on Test dataset')\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gC0amBy5CIx1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gC0amBy5CIx1",
        "outputId": "252ed4ef-c5e0-49ab-ca9c-5f9ace5da0df"
      },
      "outputs": [],
      "source": [
        "product_test = df_test.groupby(['encoded_product_type', 'product_type', 'date'])[['sales', 'predicted_sales']].mean().reset_index()\n",
        "product_df = product_test\n",
        "fig = make_subplots(rows=11, cols=3, subplot_titles=[f'{product}' for product in product_test.product_type.unique()])\n",
        "n=0\n",
        "for row in range (1,12):\n",
        "  for col in range(1,4):\n",
        "    df = product_df[product_df['encoded_product_type'] == n]\n",
        "    n += 1\n",
        "\n",
        "    px_fig = px.line(df, x='date', y='sales')\n",
        "    px_fig.add_scatter(x=df['date'], y=df['predicted_sales'], mode='lines')\n",
        "\n",
        "    for trace in px_fig['data']:\n",
        "      fig.add_trace(trace, row=row, col=col)\n",
        "\n",
        "fig.update_layout(height=4000, width=2000, title_text = 'Sales Prediction by Product on Test dataset')\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4hHsiYzJo8u2",
      "metadata": {
        "id": "4hHsiYzJo8u2"
      },
      "source": [
        "Aggregate the MAE metrics into each Store and Product to get a better granular performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I5jEmxe72chN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5jEmxe72chN",
        "outputId": "527fecc0-d906-44ad-d583-511330c8067c"
      },
      "outputs": [],
      "source": [
        "# Create a mapping from encoded_product_type to product_type\n",
        "product_mapping = df_train.set_index('encoded_product_type')['product_type'].to_dict()\n",
        "\n",
        "# Calculate and print MAE by store\n",
        "for store_nbr in df_train['store_nbr'].unique():\n",
        "    # Subset the data\n",
        "    X_train_subset = X_train_all.loc[X_train_all['store_nbr'] == store_nbr, ['special_offer', 'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']]\n",
        "    y_train_subset = y_train.loc[X_train_subset.index]\n",
        "    X_test_subset = X_test_all.loc[X_test_all['store_nbr'] == store_nbr, ['special_offer', 'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']]\n",
        "    y_test_subset = y_test.loc[X_test_subset.index]\n",
        "\n",
        "    # Make predictions\n",
        "    train_preds_subset = model.predict(X_train_subset)\n",
        "    test_preds_subset = model.predict(X_test_subset)\n",
        "\n",
        "    # Calculate MAE\n",
        "    train_mae_subset = mean_absolute_error(y_train_subset, train_preds_subset)\n",
        "    test_mae_subset = mean_absolute_error(y_test_subset, test_preds_subset)\n",
        "\n",
        "    # Print MAE\n",
        "    print(f'MAE on train set for store {store_nbr}: {train_mae_subset}')\n",
        "    print(f'MAE on test set for store {store_nbr}: {test_mae_subset}\\n')\n",
        "\n",
        "# Calculate and print MAE by product\n",
        "for encoded_product_type in df_train['encoded_product_type'].unique():\n",
        "    # Subset the data\n",
        "    X_train_subset = X_train[X_train['encoded_product_type'] == encoded_product_type]\n",
        "    y_train_subset = y_train.loc[X_train_subset.index]\n",
        "    X_test_subset = X_test[X_test['encoded_product_type'] == encoded_product_type]\n",
        "    y_test_subset = y_test.loc[X_test_subset.index]\n",
        "\n",
        "    # Make predictions\n",
        "    train_preds_subset = model.predict(X_train_subset)\n",
        "    test_preds_subset = model.predict(X_test_subset)\n",
        "\n",
        "    # Calculate MAE\n",
        "    train_mae_subset = mean_absolute_error(y_train_subset, train_preds_subset)\n",
        "    test_mae_subset = mean_absolute_error(y_test_subset, test_preds_subset)\n",
        "\n",
        "    # Get the corresponding product_type\n",
        "    product_type = product_mapping.get(encoded_product_type, 'Unknown')\n",
        "\n",
        "    # Print MAE\n",
        "    print(f'MAE on train set for product {product_type}: {train_mae_subset}')\n",
        "    print(f'MAE on test set for product {product_type}: {test_mae_subset}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9pbA3TLe2pgS",
      "metadata": {
        "id": "9pbA3TLe2pgS"
      },
      "outputs": [],
      "source": [
        "#import to excel file format\n",
        "\n",
        "# Initialize an empty DataFrame to store the results\n",
        "results = pd.DataFrame(columns=['Store/Product', 'Train MAE', 'Test MAE'])\n",
        "\n",
        "# Calculate and print MAE by store\n",
        "for store_nbr in df_train['store_nbr'].unique():\n",
        "   # Subset the data\n",
        "    X_train_subset = X_train_all.loc[X_train_all['store_nbr'] == store_nbr, ['special_offer', 'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']]\n",
        "    y_train_subset = y_train.loc[X_train_subset.index]\n",
        "    X_test_subset = X_test_all.loc[X_test_all['store_nbr'] == store_nbr, ['special_offer', 'encoded_product_type','sales_lag365','sales_lag21','rolling_means7']]\n",
        "    y_test_subset = y_test.loc[X_test_subset.index]\n",
        "\n",
        "    # Make predictions\n",
        "    train_preds_subset = model.predict(X_train_subset)\n",
        "    test_preds_subset = model.predict(X_test_subset)\n",
        "\n",
        "    # Calculate MAE\n",
        "    train_mae_subset = mean_absolute_error(y_train_subset, train_preds_subset)\n",
        "    test_mae_subset = mean_absolute_error(y_test_subset, test_preds_subset)\n",
        "\n",
        "    # Append to results DataFrame\n",
        "    results = results.append({'Store/Product': f'Store {store_nbr}', 'Train MAE': train_mae_subset, 'Test MAE': test_mae_subset}, ignore_index=True)\n",
        "\n",
        "# Calculate and print MAE by product\n",
        "for encoded_product_type in df_train['encoded_product_type'].unique():\n",
        "    # Subset the data\n",
        "    X_train_subset = X_train[X_train['encoded_product_type'] == encoded_product_type]\n",
        "    y_train_subset = y_train.loc[X_train_subset.index]\n",
        "    X_test_subset = X_test[X_test['encoded_product_type'] == encoded_product_type]\n",
        "    y_test_subset = y_test.loc[X_test_subset.index]\n",
        "\n",
        "    # Make predictions\n",
        "    train_preds_subset = model.predict(X_train_subset)\n",
        "    test_preds_subset = model.predict(X_test_subset)\n",
        "\n",
        "    # Calculate MAE\n",
        "    train_mae_subset = mean_absolute_error(y_train_subset, train_preds_subset)\n",
        "    test_mae_subset = mean_absolute_error(y_test_subset, test_preds_subset)\n",
        "\n",
        "    # Get the corresponding product_type\n",
        "    product_type = product_mapping.get(encoded_product_type, 'Unknown')\n",
        "\n",
        "    # Append to results DataFrame\n",
        "    results = results.append({'Store/Product': f'Product {product_type}', 'Train MAE': train_mae_subset, 'Test MAE': test_mae_subset}, ignore_index=True)\n",
        "\n",
        "# Export to Excel\n",
        "results.to_excel('mae_results.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cv7WkqjDAFqX",
      "metadata": {
        "id": "Cv7WkqjDAFqX"
      },
      "outputs": [],
      "source": [
        "# Average prediction of sales by store and product\n",
        "\n",
        "avgPredStore = df_test.groupby('store_nbr')['predicted_sales'].mean().reset_index()\n",
        "avgPredStore.to_excel('avg_store.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X7QlvHrgATye",
      "metadata": {
        "id": "X7QlvHrgATye"
      },
      "outputs": [],
      "source": [
        "avgPredProd = df_test.groupby('product_type')['predicted_sales'].mean().reset_index()\n",
        "avgPredProd.to_excel('avg_product.xlsx', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
